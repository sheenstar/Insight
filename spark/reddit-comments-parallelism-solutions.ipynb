{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "from time import time\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Spark will usually split your data into a set of partitions automatically but there are cases where you want to do this manually to improve performance. You can us the repartition() method to define the number of partitions in your RDD. In this tutorial we explore the number of partitions on performance.\n",
    "\n",
    "We are going to be using the Reddit comments dataset for this tutorial. More information on this dataset can be found [here](https://sites.google.com/a/insightdatascience.com/spark-lab/s3-data/reddit-comments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = [StructField(\"archived\", BooleanType(), True),\n",
    "        StructField(\"author\", StringType(), True),\n",
    "        StructField(\"author_flair_css_class\", StringType(), True),\n",
    "        StructField(\"body\", StringType(), True),\n",
    "        StructField(\"controversiality\", LongType(), True),\n",
    "        StructField(\"created_utc\", StringType(), True),\n",
    "        StructField(\"day\", LongType(), True),\n",
    "        StructField(\"distinguished\", StringType(), True),\n",
    "        StructField(\"downs\", LongType(), True),\n",
    "        StructField(\"edited\", StringType(), True),\n",
    "        StructField(\"gilded\", LongType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"link_id\", StringType(), True),\n",
    "        StructField(\"month\", LongType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"parent_id\", StringType(), True),\n",
    "        StructField(\"retrieved_on\", LongType(), True),\n",
    "        StructField(\"score\", LongType(), True),\n",
    "        StructField(\"score_hidden\", BooleanType(), True),\n",
    "        StructField(\"subreddit\", StringType(), True),\n",
    "        StructField(\"subreddit_id\", StringType(), True),\n",
    "        StructField(\"ups\", LongType(), True),\n",
    "        StructField(\"year\", LongType(), True)]\n",
    "#rawDF = sqlContext.read.json(\"s3a://reddit-comments/2008\", StructType(fields))\n",
    "rawDF = sqlContext.read.parquet(\"s3a://reddit-comments-parquet/year=2008\")\n",
    "rawDF.persist(StorageLevel.DISK_ONLY)\n",
    "rawDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism - example 1\n",
    "\n",
    "To analyze the performance of a simple Spark job with different numbers of partitions, let's split our data into varying numbers of partitions. We'll persist the partitions to save recomputing down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting up DataFrames to have various number of partitions ranging from 2 to 32\n",
    "# All will be persisted in memory and the same job will be run on each one to show \n",
    "# performance benefits of different number of partitions\n",
    "repart_2_df = rawDF.repartition(2).persist(StorageLevel.DISK_ONLY)\n",
    "repart_4_df = rawDF.repartition(4).persist(StorageLevel.DISK_ONLY)\n",
    "repart_8_df = rawDF.repartition(8).persist(StorageLevel.DISK_ONLY)\n",
    "repart_16_df = rawDF.repartition(16).persist(StorageLevel.DISK_ONLY)\n",
    "repart_32_df = rawDF.repartition(32).persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform a simple job (count the number of elements in each dataframe) and compare how long the operation takes for various number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run through each DataFrame and count the number of elements in each. This will \n",
    "# trigger the DataFrames to persist into Memory and Disk\n",
    "df_arr = [(repart_2_df, 2), \n",
    "          (repart_4_df, 4), \n",
    "          (repart_8_df, 8), \n",
    "          (repart_16_df, 16), \n",
    "          (repart_32_df, 32)]\n",
    "for df in df_arr:\n",
    "    start_time = time()\n",
    "    df[0].count()\n",
    "    end_time = time()\n",
    "    print \"{} partitions took {} seconds to repartition and count\".format(df[1], end_time - start_time) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the runtime indicate about the scheduling and distribution of tasks to worker nodes?\n",
    "\n",
    "### Look at 4040 to see how the count action varied in time with increased number of partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism - example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function sorts an array of UTC time and calculates the delta time in days between each consecutive UTC time\n",
    "# Returns a Row object containing the subreddit, the median time delta representing the median time in days \n",
    "# between comments for any subreddit, and the total number of comments in each subreddit\n",
    "def calc_median(row):\n",
    "    from heapq import heappop\n",
    "    \n",
    "    subreddit = row[0]\n",
    "    val_arr = row[1]\n",
    "    num_comments = len(val_arr)\n",
    "    \n",
    "    dt = []\n",
    "\n",
    "    if len(val_arr) > 1:\n",
    "        prev_val = heappop(val_arr)\n",
    "        while len(val_arr) > 0:\n",
    "            curr_val = heappop(val_arr)\n",
    "            dt.append(curr_val - prev_val)\n",
    "            prev_val = curr_val\n",
    "        return Row(subreddit=subreddit, median_time_days=float(median(dt))/60/60/24, num_comments=num_comments)\n",
    "    else:\n",
    "        return Row(subreddit=subreddit, median_time_days=0, num_comments=num_comments)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code is very inefficient. Can you figure out why (before attempting to run it)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combiner(value):\n",
    "    return value\n",
    "\n",
    "def merger(x, value):\n",
    "    from heapq import heappush\n",
    "\n",
    "    heappush(x, value[0])\n",
    "\n",
    "    return x\n",
    "\n",
    "def merge_combiner(x, y):\n",
    "    from heapq import heappush, heappop\n",
    "\n",
    "    while len(y) > 0:\n",
    "        heappush(x, heappop(y))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop through each DataFrame and run the same job to calculate the median time between comments for each\n",
    "# subreddit\n",
    "for df in df_arr:\n",
    "    start_time = time()\n",
    "    curr_df = df[0]\n",
    "    num_partitions = df[1]\n",
    "\n",
    "    subreddit_comment_times = curr_df.rdd.map(lambda r: (r.subreddit, [int(r.created_utc)]))\n",
    "    median_time_between_posts_df = subreddit_comment_times.combineByKey(combiner, merger, merge_combiner)\\\n",
    "                                                          .map(calc_median)\\\n",
    "                                                          .toDF()\n",
    "\n",
    "    total_cnt = median_time_between_posts_df.count()\n",
    "    end_time = time()\n",
    "    print \"{} partitions took {} seconds to process, {} final records\".format(df[1], end_time - start_time, total_cnt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Revised code using predicate pushdown\n",
    "for df in df_arr:\n",
    "    start_time = time()\n",
    "    curr_df = df[0]\n",
    "    num_partitions = df[1]\n",
    "\n",
    "    subreddit_comment_times = curr_df.select(['subreddit', 'created_utc']).rdd.map(lambda r: (r.subreddit, [int(r.created_utc)]))\n",
    "    median_time_between_posts_df = subreddit_comment_times.combineByKey(combiner, merger, merge_combiner)\\\n",
    "                                                          .map(calc_median)\\\n",
    "                                                          .toDF()\n",
    "\n",
    "    total_cnt = median_time_between_posts_df.count()\n",
    "    end_time = time()\n",
    "    print \"{} partitions took {} seconds to process, {} final records\".format(df[1], end_time - start_time, total_cnt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Dataframes use predicate pushdow and will permit users to filter before running other map transformations. This reduces the total amount of data going into the map transformation which is particularly useful for DataFrames with many many columns. Here we simply need to select the 2 columns of interest, \"subreddit\" and \"created_utc\" before mapping the DataFrame back to a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A scatter plot where each element is a subreddit. We can see that subreddits with more than 40 comments tend to\n",
    "# also have more frequent comment activity throughout the year\n",
    "median_time_between_posts_pd = median_time_between_posts_df.toPandas()\n",
    "median_time_between_posts_pd.plot(kind='scatter', x='num_comments', y='median_time_days', xlim=(-5, 100), ylim=(-5, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: What is the optimal number of partitions for calculating the authors who have written the longest comments per subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.udf.register('length_text', lambda x: len(x), LongType())\n",
    "\n",
    "partition_list = [16,50,100,500,1000,2500,5000]\n",
    "elapsed_time = []\n",
    "\n",
    "# Loop through each DataFrame and run the same job to calculate the median time between comments for each\n",
    "# subreddit\n",
    "for partitions in partition_list:\n",
    "    repart_df = rawDF.repartition(partitions).persist(StorageLevel.DISK_ONLY)\n",
    "    repart_df.registerTempTable(\"repart\")\n",
    "    repart_df.count()\n",
    "    \n",
    "    start_time = time()\n",
    "\n",
    "    max_comment_len_per_subreddit_author = sqlContext.sql(\"\"\"\n",
    "        SELECT subreddit, \n",
    "            author,\n",
    "            MAX(length_text(body)) as longest_comment\n",
    "        FROM repart\n",
    "        GROUP BY subreddit, author\n",
    "    \"\"\")\n",
    "    max_comment_len_per_subreddit_author.persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "    max_comment_len_per_subreddit_author.registerTempTable(\"max_comment\")\n",
    "    max_comment_len_per_subreddit = sqlContext.sql(\"\"\"\n",
    "        SELECT max_comment.subreddit, \n",
    "               max_comment.author, \n",
    "               all_time_longest\n",
    "        FROM max_comment, (\n",
    "            SELECT subreddit, \n",
    "                   MAX(longest_comment) as all_time_longest\n",
    "            FROM max_comment\n",
    "            GROUP BY subreddit\n",
    "        ) as all_max_comment\n",
    "        WHERE max_comment.longest_comment = all_max_comment.all_time_longest\n",
    "          AND max_comment.subreddit = all_max_comment.subreddit\n",
    "        ORDER BY all_time_longest DESC\n",
    "    \"\"\")\n",
    "    num_records = max_comment_len_per_subreddit.count()\n",
    "    elapsed_time.append(time()-start_time)\n",
    "    \n",
    "    # clean up before next iteration\n",
    "    repart_df.unpersist()\n",
    "    max_comment_len_per_subreddit_author.unpersist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "benchmark = DataFrame({\"partitions\": partition_list,\n",
    "                       \"elapsed_time\": elapsed_time})\n",
    "benchmark.plot(x='partitions', y='elapsed_time', logx=True)\n",
    "plt.ylabel('time (s)')\n",
    "plt.xlabel('number of partitions')\n",
    "print benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
